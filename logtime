/usr/bin/python3.8 /home/user/football/HRL/MAIN.py
Environment initialized
action_dim=[2, 2, 2]
obs_dim=[43, 43, 43]
Successfully load mmoe model. model_path:./models/coach/model_mmoe_100_10step
start runner.run()
/home/user/football/HRL/rsoccer_gym/vss/env_ma/vss_gym_ma.py:205: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  observations = np.array(observations)

ODE Message 3: LCP internal error, s <= 0 (s=-0.0000e+00)
============epi=1,step=300,avg_reward=-2.246400649379797,goal_score=0==============
[('env.step', 1.028017282485962), ('update goal', 0.062487125396728516), ('store to buffer', 0.02634119987487793), ('init episode', 0.009819746017456055), ('write log', 0.00034427642822265625), ('train agents', 0.0002651214599609375), ('train coach', 1.430511474609375e-06), ('save model', 7.152557373046875e-07)]
============epi=2,step=600,avg_reward=-2.5260117658642587,goal_score=0==============
[('env.step', 2.0391366481781006), ('update goal', 0.12408685684204102), ('store to buffer', 0.06964397430419922), ('init episode', 0.018055200576782227), ('write log', 0.0006649494171142578), ('train agents', 0.0005273818969726562), ('train coach', 3.0994415283203125e-06), ('save model', 1.1920928955078125e-06)]
============epi=3,step=900,avg_reward=-1.988987485354034,goal_score=0==============
[('env.step', 3.0457205772399902), ('update goal', 0.18433308601379395), ('store to buffer', 0.09644365310668945), ('init episode', 0.025142431259155273), ('write log', 0.0010199546813964844), ('train agents', 0.0007810592651367188), ('train coach', 4.76837158203125e-06), ('save model', 1.9073486328125e-06)]
============epi=4,step=1200,avg_reward=-1.9362952891347573,goal_score=0==============
[('train agents', 7.829241514205933), ('env.step', 4.117123365402222), ('update goal', 0.24581241607666016), ('store to buffer', 0.145707368850708), ('init episode', 0.03298068046569824), ('write log', 0.0011942386627197266), ('train coach', 6.9141387939453125e-06), ('save model', 2.86102294921875e-06)]
============epi=5,step=1500,avg_reward=-1.6856131229650417,goal_score=0==============
[('train agents', 14.570670127868652), ('env.step', 5.5175182819366455), ('update goal', 0.31943440437316895), ('store to buffer', 0.22671842575073242), ('init episode', 0.04290437698364258), ('train coach', 0.008011579513549805), ('write log', 0.0014829635620117188), ('save model', 5.0067901611328125e-06)]
============epi=6,step=1800,avg_reward=-1.0514556409538645,goal_score=0==============
[('train agents', 21.555333375930786), ('env.step', 7.01725697517395), ('update goal', 0.39812207221984863), ('store to buffer', 0.29960083961486816), ('init episode', 0.053105831146240234), ('train coach', 0.015184879302978516), ('write log', 0.001771688461303711), ('save model', 7.3909759521484375e-06)]
============epi=7,step=2100,avg_reward=-1.4392646918458347,goal_score=0==============
[('train agents', 28.224200963974), ('env.step', 8.383965015411377), ('update goal', 0.47355175018310547), ('store to buffer', 0.37419605255126953), ('init episode', 0.06419515609741211), ('train coach', 0.02259540557861328), ('write log', 0.002070903778076172), ('save model', 9.5367431640625e-06)]
============epi=8,step=2400,avg_reward=-2.4826363540715928,goal_score=0==============
[('train agents', 35.05364108085632), ('env.step', 10.085254907608032), ('update goal', 0.5485906600952148), ('store to buffer', 0.4460775852203369), ('init episode', 0.07426738739013672), ('train coach', 0.03009819984436035), ('write log', 0.0023102760314941406), ('save model', 1.1920928955078125e-05)]
============epi=9,step=2700,avg_reward=-1.2688192002900713,goal_score=0==============
[('train agents', 41.76676893234253), ('env.step', 11.480997800827026), ('update goal', 0.6244242191314697), ('store to buffer', 0.5216023921966553), ('init episode', 0.0843040943145752), ('train coach', 0.03785991668701172), ('write log', 0.0026023387908935547), ('save model', 1.4066696166992188e-05)]
============epi=10,step=3000,avg_reward=-1.0967754222739365,goal_score=0==============
[('train agents', 48.43997621536255), ('env.step', 12.912144899368286), ('update goal', 0.6999773979187012), ('store to buffer', 0.5972068309783936), ('init episode', 0.09444427490234375), ('train coach', 0.045324087142944336), ('write log', 0.0028963088989257812), ('save model', 1.6450881958007812e-05)]
============epi=11,step=3300,avg_reward=-1.6894955605910573,goal_score=0==============
[('train agents', 55.203492879867554), ('env.step', 14.56838083267212), ('update goal', 0.7749655246734619), ('store to buffer', 0.6746878623962402), ('init episode', 0.10458135604858398), ('train coach', 0.052555084228515625), ('write log', 0.003190279006958008), ('save model', 1.8835067749023438e-05)]
============epi=12,step=3600,avg_reward=-1.5702065673672563,goal_score=0==============
[('train agents', 62.041343688964844), ('env.step', 16.147357940673828), ('update goal', 0.8513598442077637), ('store to buffer', 0.7559108734130859), ('init episode', 0.11556506156921387), ('train coach', 0.05996990203857422), ('write log', 0.003484964370727539), ('save model', 2.1219253540039062e-05)]
============epi=13,step=3900,avg_reward=-0.8495806755295218,goal_score=0==============
[('train agents', 68.79386806488037), ('env.step', 17.50524401664734), ('update goal', 0.9298970699310303), ('store to buffer', 0.8378334045410156), ('init episode', 0.1255941390991211), ('train coach', 0.06743717193603516), ('write log', 0.0037260055541992188), ('save model', 2.3365020751953125e-05)]
============epi=14,step=4200,avg_reward=-1.4834154657621077,goal_score=0==============
[('train agents', 75.59891247749329), ('env.step', 19.297849655151367), ('update goal', 1.0061733722686768), ('store to buffer', 0.9107983112335205), ('init episode', 0.13580584526062012), ('train coach', 0.07487225532531738), ('write log', 0.004024982452392578), ('save model', 2.574920654296875e-05)]
============epi=15,step=4500,avg_reward=-1.8126082100999137,goal_score=0==============
[('train agents', 82.44266104698181), ('env.step', 20.72758436203003), ('update goal', 1.080636739730835), ('store to buffer', 0.9864110946655273), ('init episode', 0.14588189125061035), ('train coach', 0.08249044418334961), ('write log', 0.00432586669921875), ('save model', 2.86102294921875e-05)]
============epi=16,step=4800,avg_reward=-2.616473010580329,goal_score=0==============
[('train agents', 89.19999957084656), (env.step', 22.24796772003174), ('update goal', 1.1558310985565186), ('store to buffer', 1.0602359771728516), ('init episode', 0.15601372718811035), ('train coach', 0.09001731872558594), ('write log', 0.0046234130859375), ('save model', 3.0994415283203125e-05)]
============epi=17,step=5100,avg_reward=-0.5961942903783989,goal_score=0==============
[('train agents', 95.90815925598145), ('env.step', 23.633123636245728), ('update goal', 1.2308893203735352), ('store to buffer', 1.135707139968872), ('init episode', 0.16626977920532227), ('train coach', 0.09742927551269531), ('write log', 0.0048673152923583984), ('save model', 3.2901763916015625e-05)]
============epi=18,step=5400,avg_reward=-1.8753809063607856,goal_score=0==============
[('train agents', 102.6836450099945), ('env.step', 25.041714906692505), ('update goal', 1.3065862655639648), ('store to buffer', 1.2131984233856201), ('init episode', 0.17634844779968262), ('train coach', 0.10471343994140625), ('write log', 0.005150556564331055), ('save model', 3.528594970703125e-05)]
============epi=19,step=5700,avg_reward=-1.0528909960497352,goal_score=0==============
[('train agents', 109.54983425140381), ('env.step', 26.502304553985596), ('update goal', 1.381871223449707), ('store to buffer', 1.284895658493042), ('init episode', 0.18615102767944336), ('train coach', 0.11171460151672363), ('write log', 0.005373954772949219), ('save model', 3.743171691894531e-05)]
============epi=20,step=6000,avg_reward=-0.6760059205766489,goal_score=0==============
[('train agents', 116.45525431632996), ('env.step', 27.952541828155518), ('update goal', 1.457000732421875), ('store to buffer', 1.3539679050445557), ('init episode', 0.19585967063903809), ('train coach', 0.11944031715393066), ('write log', 0.005602359771728516), ('save model', 3.981590270996094e-05)]
============epi=21,step=6300,avg_reward=-0.8663810516339306,goal_score=0==============
[('train agents', 123.31944370269775), ('env.step', 29.327337503433228), ('update goal', 1.5329654216766357), ('store to buffer', 1.4248239994049072), ('init episode', 0.20601415634155273), ('train coach', 0.12691450119018555), ('write log', 0.00586247444152832), ('save model', 4.220008850097656e-05)]
============epi=22,step=6600,avg_reward=-0.8556532607858834,goal_score=0==============
[('train agents', 130.38972806930542), ('env.step', 30.73857831954956), ('update goal', 1.6109421253204346), ('store to buffer', 1.5006461143493652), ('init episode', 0.2162001132965088), ('train coach', 0.13417530059814453), ('write log', 0.0061457157135009766), ('save model', 4.482269287109375e-05)]
============epi=23,step=6900,avg_reward=-1.242469403187939,goal_score=0==============
[('train agents', 138.6064419746399), ('env.step', 33.64636278152466), ('update goal', 1.7008724212646484), ('store to buffer', 1.5821406841278076), ('init episode', 0.2264087200164795), ('train coach', 0.1415119171142578), ('write log', 0.006434202194213867), ('save model', 4.7206878662109375e-05)]
============epi=24,step=6955,avg_reward=-9.487276760269596,goal_score=-1==============
[('train agents', 140.04742765426636), ('env.step', 33.945921659469604), ('update goal', 1.7159569263458252), ('store to buffer', 1.5966215133666992), ('init episode', 0.23655080795288086), ('train coach', 0.15047168731689453), ('write log', 0.0066928863525390625), ('save model', 4.935264587402344e-05)]
============epi=25,step=7010,avg_reward=-7.188636099098383,goal_score=-1==============
[('train agents', 141.41824173927307), ('env.step', 34.2199285030365), ('update goal', 1.7294695377349854), ('store to buffer', 1.6125357151031494), ('init episode', 0.24915432929992676), ('train coach', 0.15784621238708496), ('write log', 0.0069658756256103516), ('save model', 5.125999450683594e-05)]
============epi=26,step=7310,avg_reward=-0.5996719697536517,goal_score=0==============
[('train agents', 150.6232681274414), ('env.step', 35.87419366836548), ('update goal', 1.8301136493682861), ('store to buffer', 1.6948730945587158), ('init episode', 0.25930190086364746), ('train coach', 0.16523051261901855), ('write log', 0.00719141960144043), ('save model', 5.340576171875e-05)]
============epi=27,step=7610,avg_reward=-0.45115518502773927,goal_score=0==============
[('train agents', 158.02832055091858), ('env.step', 37.334065198898315), ('update goal', 1.9107615947723389), ('store to buffer', 1.7759323120117188), ('init episode', 0.2695155143737793), ('train coach', 0.17287993431091309), ('write log', 0.00731658935546875), ('save model', 5.5789947509765625e-05)]
============epi=28,step=7910,avg_reward=-0.7828673138648661,goal_score=0==============
[('train agents', 165.4625265598297), ('env.step', 38.8525652885437), ('update goal', 1.9928700923919678), ('store to buffer', 1.8485543727874756), ('init episode', 0.2796459197998047), ('train coach', 0.1803112030029297), ('write log', 0.007444620132446289), ('save model', 5.7697296142578125e-05)]
============epi=29,step=8210,avg_reward=-0.44617534294089994,goal_score=0==============
[('train agents', 173.47575497627258), ('env.step', 40.3920464515686), ('update goal', 2.0736260414123535), ('store to buffer', 1.9271385669708252), ('init episode', 0.28998541831970215), ('train coach', 0.18792104721069336), ('write log', 0.007734775543212891), ('save model', 6.008148193359375e-05)]
============epi=30,step=8510,avg_reward=-0.3144481886025833,goal_score=0==============
[('train agents', 180.49267673492432), ('env.step', 41.84559655189514), ('update goal', 2.1492109298706055), ('store to buffer', 1.9983670711517334), ('init episode', 0.3004636764526367), ('train coach', 0.19534015655517578), ('write log', 0.007982254028320312), ('save model', 6.222724914550781e-05)]
============epi=31,step=8810,avg_reward=-0.48232621767426376,goal_score=0==============
[('train agents', 188.20121026039124), ('env.step', 43.37909770011902), ('update goal', 2.232747793197632), ('store to buffer', 2.073201894760132), ('init episode', 0.3102729320526123), ('train coach', 0.2068789005279541), ('write log', 0.008351564407348633), ('save model', 6.556510925292969e-05)]
============epi=32,step=9110,avg_reward=-0.4114714302573107,goal_score=0==============
[('train agents', 196.09086847305298), ('env.step', 44.887696981430054), ('update goal', 2.3147060871124268), ('store to buffer', 2.151942491531372), ('init episode', 0.3240935802459717), ('train coach', 0.21429944038391113), ('write log', 0.008604049682617188), ('save model', 6.794929504394531e-05)]
============epi=33,step=9410,avg_reward=-0.5957691727123616,goal_score=0==============
[('train agents', 205.48014163970947), ('env.step', 46.57564449310303), ('update goal', 2.4115774631500244), ('store to buffer', 2.2387397289276123), ('init episode', 0.3341655731201172), ('train coach', 0.22179388999938965), ('write log', 0.008901596069335938), ('save model', 7.033348083496094e-05)]
============epi=34,step=9710,avg_reward=-1.5007594748847193,goal_score=0==============
[('train agents', 212.81005787849426), ('env.step', 48.1308057308197), ('update goal', 2.4887075424194336), ('store to buffer', 2.312589168548584), ('init episode', 0.3442070484161377), ('train coach', 0.22899436950683594), ('write log', 0.009145975112915039), ('save model', 7.2479248046875e-05)]
============epi=35,step=10010,avg_reward=-0.9950268820811129,goal_score=0==============
[('train agents', 220.33048677444458), ('env.step', 49.85854434967041), ('update goal', 2.5687568187713623), ('store to buffer', 2.3867478370666504), ('init episode', 0.35399770736694336), ('train coach', 0.23860788345336914), ('write log', 0.009386301040649414), ('save model', 7.486343383789062e-05)]
============epi=36,step=10310,avg_reward=-1.0700127455360182,goal_score=0==============
[('train agents', 227.96649384498596), ('env.step', 51.56015634536743), ('update goal', 2.64728045463562), ('store to buffer', 2.4614462852478027), ('init episode', 0.36551332473754883), ('train coach', 0.24588847160339355), ('write log', 0.009667158126831055), ('save model', 7.677078247070312e-05)]
============epi=37,step=10610,avg_reward=-0.9771357070493685,goal_score=0==============
[('train agents', 235.93404936790466), ('env.step', 53.09112358093262), ('update goal', 2.7316501140594482), ('store to buffer', 2.5412850379943848), ('init episode', 0.37564539909362793), ('train coach', 0.2533257007598877), ('write log', 0.009975194931030273), ('save model', 7.939338684082031e-05)]
============epi=38,step=10910,avg_reward=-1.6668106351151177,goal_score=0==============
[('train agents', 244.87216544151306), ('env.step', 54.726462602615356), ('update goal', 2.8321096897125244), ('store to buffer', 2.6231253147125244), ('init episode', 0.38575077056884766), ('train coach', 0.2612781524658203), ('write log', 0.01053476333618164), ('save model', 8.249282836914062e-05)]
============epi=39,step=11210,avg_reward=-0.8427530588053781,goal_score=0==============
[('train agents', 253.74429297447205), ('env.step', 56.5266330242157), ('update goal', 2.9227566719055176), ('store to buffer', 2.704782485961914), ('init episode', 0.3958592414855957), ('train coach', 0.2689077854156494), ('write log', 0.010825157165527344), ('save model', 8.559226989746094e-05)]
============epi=40,step=11510,avg_reward=-0.9391123839608232,goal_score=0==============
[('train agents', 262.016389131546), ('env.step', 58.24082827568054), ('update goal', 3.0048530101776123), ('store to buffer', 2.7818763256073), ('init episode', 0.4089944362640381), ('train coach', 0.27708005905151367), ('write log', 0.011170148849487305), ('save model', 8.797645568847656e-05)]
============epi=41,step=11810,avg_reward=-1.5271702998961665,goal_score=0==============
[('train agents', 271.3494555950165), ('env.step', 59.942970275878906), ('update goal', 3.097322940826416), ('store to buffer', 2.8873140811920166), ('init episode', 0.41977691650390625), ('train coach', 0.29100584983825684), ('write log', 0.011569976806640625), ('save model', 9.250640869140625e-05)]
============epi=42,step=12110,avg_reward=-0.5236612931263587,goal_score=0==============
[('train agents', 280.1760380268097), ('env.step', 61.65735173225403), ('update goal', 3.1929681301116943), ('store to buffer', 2.9674994945526123), ('init episode', 0.4400966167449951), ('train coach', 0.3008711338043213), ('write log', 0.011709451675415039), ('save model', 9.489059448242188e-05)]
============epi=43,step=12410,avg_reward=-0.48296411919647,goal_score=0==============
[('train agents', 289.04820346832275), ('env.step', 63.274115562438965), ('update goal', 3.283815622329712), ('store to buffer', 3.0619757175445557), ('init episode', 0.4540524482727051), ('train coach', 0.3082857131958008), ('write log', 0.012012481689453125), ('save model', 9.72747802734375e-05)]

ODE Message 3: LCP internal error, s <= 0 (s=-0.0000e+00)
============epi=44,step=12710,avg_reward=-2.6066642617247644,goal_score=0==============
[('train agents', 299.0004315376282), ('env.step', 65.57095956802368), ('update goal', 3.383077383041382), ('store to buffer', 3.156385898590088), ('init episode', 0.4641914367675781), ('train coach', 0.3156251907348633), ('write log', 0.012135744094848633), ('save model', 9.918212890625e-05)]
============epi=45,step=13010,avg_reward=0.21892313394039883,goal_score=0==============
[('train agents', 310.7993767261505), ('env.step', 67.54158425331116), ('update goal', 3.484210729598999), ('store to buffer', 3.2920053005218506), ('init episode', 0.4746863842010498), ('train coach', 0.3247861862182617), ('write log', 0.01249241828918457), ('save model', 0.00010228157043457031)]
============epi=46,step=13310,avg_reward=-0.38058490942492684,goal_score=0==============
[('train agents', 319.6356613636017), ('env.step', 69.25020217895508), ('update goal', 3.5737855434417725), ('store to buffer', 3.375714063644409), ('init episode', 0.4961414337158203), ('train coach', 0.33231353759765625), ('write log', 0.012806177139282227), ('save model', 0.00010538101196289062)]
============epi=47,step=13610,avg_reward=-1.324906664049101,goal_score=0==============
[('train agents', 328.19931411743164), ('env.step', 70.84776520729065), ('update goal', 3.663870096206665), ('store to buffer', 3.4557695388793945), ('init episode', 0.5066730976104736), ('train coach', 0.33976316452026367), ('write log', 0.013105154037475586), ('save model', 0.00010776519775390625)]
============epi=48,step=13910,avg_reward=-0.7937134817828921,goal_score=0==============
[('train agents', 335.89297437667847), ('env.step', 72.53366041183472), ('update goal', 3.7444798946380615), ('store to buffer', 3.5343642234802246), ('init episode', 0.5167083740234375), ('train coach', 0.3474113941192627), ('write log', 0.01338338851928711), ('save model', 0.00011014938354492188)]
============epi=49,step=14210,avg_reward=-1.6047846963341699,goal_score=0==============
[('train agents', 344.28524136543274), ('env.step', 74.17647743225098), ('update goal', 3.831521987915039), ('store to buffer', 3.6097936630249023), ('init episode', 0.5267024040222168), ('train coach', 0.355224609375), ('write log', 0.013685464859008789), ('save model', 0.00011277198791503906)]
============epi=50,step=14510,avg_reward=-0.5857611641342934,goal_score=0==============
[('train agents', 354.1619460582733), ('env.step', 76.02800464630127), ('update goal', 3.9325015544891357), ('store to buffer', 3.705810308456421), ('init episode', 0.5374917984008789), ('train coach', 0.3626213073730469), ('write log', 0.013962030410766602), ('save model', 0.00011539459228515625)]
============epi=51,step=14612,avg_reward=-5.658087278911503,goal_score=-1==============
[('train agents', 357.07886362075806), ('env.step', 76.59653067588806), ('update goal', 3.963508129119873), ('store to buffer', 3.7315146923065186), ('init episode', 0.5477855205535889), ('train coach', 0.3709239959716797), ('write log', 0.014262914657592773), ('save model', 0.00011754035949707031)]
============epi=52,step=14912,avg_reward=-1.064100398744089,goal_score=0==============
[('train agents', 365.8673918247223), ('env.step', 78.30815505981445), ('update goal', 4.054934740066528), ('store to buffer', 3.8200807571411133), ('init episode', 0.5586094856262207), ('train coach', 0.37834882736206055), ('write log', 0.01450204849243164), ('save model', 0.00011992454528808594)]
============epi=53,step=15212,avg_reward=-0.8174313549945457,goal_score=0==============
[('train agents', 374.3529784679413), ('env.step', 79.89357948303223), ('update goal', 4.1428868770599365), ('store to buffer', 3.8975558280944824), ('init episode', 0.5688834190368652), ('train coach', 0.3859262466430664), ('write log', 0.01483917236328125), ('save model', 0.0001220703125)]
============epi=54,step=15512,avg_reward=-0.28520672743401554,goal_score=0==============
[('train agents', 382.2787754535675), ('env.step', 81.35209512710571), ('update goal', 4.223394155502319), ('store to buffer', 3.978242874145508), ('init episode', 0.5791499614715576), ('train coach', 0.39345455169677734), ('write log', 0.015080928802490234), ('save model', 0.0001246929168701172)]
============epi=55,step=15658,avg_reward=2.5266755686886797,goal_score=1==============
[('train agents', 386.2918071746826), ('env.step', 82.10963487625122), ('update goal', 4.264486789703369), ('store to buffer', 4.020860195159912), ('init episode', 0.5893154144287109), ('train coach', 0.4009559154510498), ('write log', 0.01536107063293457), ('save model', 0.0001270771026611328)]
============epi=56,step=15958,avg_reward=-0.46398108683059747,goal_score=0==============
[('train agents', 393.87886333465576), ('env.step', 83.63564491271973), ('update goal', 4.346070289611816), ('store to buffer', 4.092557668685913), ('init episode', 0.5998780727386475), ('train coach', 0.40833616256713867), ('write log', 0.015589237213134766), ('save model', 0.00012922286987304688)]
============epi=57,step=16258,avg_reward=-0.5039760670971906,goal_score=0==============
[('train agents', 401.1123216152191), ('env.step', 85.01297974586487), ('update goal', 4.422891139984131), ('store to buffer', 4.163802623748779), ('init episode', 0.6100964546203613), ('train coach', 0.4158601760864258), ('write log', 0.015900373458862305), ('save model', 0.00013184547424316406)]
============epi=58,step=16511,avg_reward=-2.548950763675744,goal_score=-1==============
[('train agents', 407.66338181495667), ('env.step', 86.28337931632996), ('update goal', 4.492610931396484), ('store to buffer', 4.238541841506958), ('init episode', 0.620739221572876), ('train coach', 0.4238145351409912), ('write log', 0.016216754913330078), ('save model', 0.0001342296600341797)]
============epi=59,step=16811,avg_reward=-0.5019218203825517,goal_score=0==============
[('train agents', 415.4995365142822), ('env.step', 87.86594843864441), ('update goal', 4.573352336883545), ('store to buffer', 4.3155906200408936), ('init episode', 0.6317610740661621), ('train coach', 0.4312775135040283), ('write log', 0.016506671905517578), ('save model', 0.0001366138458251953)]
============epi=60,step=17111,avg_reward=-0.48378849470381735,goal_score=0==============
[('train agents', 422.6950786113739), ('env.step', 89.27430295944214), ('update goal', 4.650026321411133), ('store to buffer', 4.38631796836853), ('init episode', 0.6419146060943604), ('train coach', 0.43879222869873047), ('write log', 0.016752958297729492), ('save model', 0.0001385211944580078)]
============epi=61,step=17411,avg_reward=-0.4837065627571484,goal_score=0==============
[('train agents', 431.31033873558044), ('env.step', 90.84590244293213), ('update goal', 4.736666679382324), ('store to buffer', 4.464827299118042), ('init episode', 0.652071475982666), ('train coach', 0.4481165409088135), ('write log', 0.017013072967529297), ('save model', 0.00014090538024902344)]
============epi=62,step=17711,avg_reward=-0.2288076790410578,goal_score=0==============
[('train agents', 440.0690793991089), ('env.step', 92.77668070793152), ('update goal', 4.827255725860596), ('store to buffer', 4.5503129959106445), ('init episode', 0.6645443439483643), ('train coach', 0.45562076568603516), ('write log', 0.01731133460998535), ('save model', 0.0001430511474609375)]
============epi=63,step=18011,avg_reward=-0.7186600034789047,goal_score=0==============
[('train agents', 448.85854482650757), ('env.step', 94.37483477592468), ('update goal', 4.93563985824585), ('store to buffer', 4.631455659866333), ('init episode', 0.6746745109558105), ('train coach', 0.4676518440246582), ('write log', 0.017466068267822266), ('save model', 0.0001456737518310547)]
============epi=64,step=18311,avg_reward=-0.022019353350290625,goal_score=0==============
[('train agents', 458.62781023979187), ('env.step', 96.37811636924744), ('update goal', 5.028406381607056), ('store to buffer', 4.724371910095215), ('init episode', 0.6946067810058594), ('train coach', 0.47893357276916504), ('write log', 0.017765522003173828), ('save model', 0.00014829635620117188)]
============epi=65,step=18611,avg_reward=-1.2143134204583896,goal_score=0==============
[('train agents', 468.01008224487305), ('env.step', 98.64389157295227), ('update goal', 5.126522779464722), ('store to buffer', 4.8179566860198975), ('init episode', 0.7098581790924072), ('train coach', 0.48658251762390137), ('write log', 0.018091917037963867), ('save model', 0.00015115737915039062)]
============epi=66,step=18834,avg_reward=-2.5565438913061436,goal_score=-1==============
[('train agents', 474.2590570449829), ('env.step', 99.86896324157715), ('update goal', 5.18991231918335), ('store to buffer', 4.880211114883423), ('init episode', 0.720526933670044), ('train coach', 0.49608278274536133), ('write log', 0.018334388732910156), ('save model', 0.00015354156494140625)]
============epi=67,step=19134,avg_reward=-0.04220984991970193,goal_score=0==============
[('train agents', 482.95200872421265), ('env.step', 101.47103476524353), ('update goal', 5.278897523880005), ('store to buffer', 4.971640110015869), ('init episode', 0.7330384254455566), ('train coach', 0.5043768882751465), ('write log', 0.01846766471862793), ('save model', 0.0001556873321533203)]
============epi=68,step=19434,avg_reward=-0.316447441623616,goal_score=0==============
[('train agents', 491.64699268341064), ('env.step', 103.18315196037292), ('update goal', 5.365872383117676), ('store to buffer', 5.059860706329346), ('init episode', 0.7447385787963867), ('train coach', 0.5149388313293457), ('write log', 0.018850088119506836), ('save model', 0.00015878677368164062)]
============epi=69,step=19566,avg_reward=2.454125682290246,goal_score=1==============
[('train agents', 495.4119243621826), ('env.step', 103.87850141525269), ('update goal', 5.404326677322388), ('store to buffer', 5.096850872039795), ('init episode', 0.7597553730010986), ('train coach', 0.5231266021728516), ('write log', 0.019086122512817383), ('save model', 0.0001609325408935547)]
============epi=70,step=19866,avg_reward=-0.7533856393445042,goal_score=0==============
[('train agents', 503.61774587631226), ('env.step', 105.62392282485962), ('update goal', 5.489205360412598), ('store to buffer', 5.174814462661743), ('init episode', 0.770179271697998), ('train coach', 0.5307230949401855), ('write log', 0.019388914108276367), ('save model', 0.00016307830810546875)]
============epi=71,step=20166,avg_reward=-0.5161968178234354,goal_score=0==============
[('train agents', 512.679445028305), ('env.step', 107.23825979232788), ('update goal', 5.5935378074646), ('store to buffer', 5.261249542236328), ('init episode', 0.7803523540496826), ('train coach', 0.539546012878418), ('write log', 0.019646406173706055), ('save model', 0.0001652240753173828)]
============epi=72,step=20466,avg_reward=-0.6052236950834897,goal_score=0==============
[('train agents', 520.8821930885315), ('env.step', 108.80670523643494), ('update goal', 5.6744208335876465), ('store to buffer', 5.347275733947754), ('init episode', 0.7909939289093018), ('train coach', 0.5472288131713867), ('write log', 0.01993870735168457), ('save model', 0.00016760826110839844)]
============epi=73,step=20766,avg_reward=-0.5881977328659016,goal_score=0==============
[('train agents', 528.8039803504944), ('env.step', 110.5027687549591), ('update goal', 5.7563018798828125), ('store to buffer', 5.4313788414001465), ('init episode', 0.8015613555908203), ('train coach', 0.55487060546875), ('write log', 0.0202333927154541), ('save model', 0.00016999244689941406)]
============epi=74,step=21066,avg_reward=-0.35991210933789997,goal_score=0==============
[('train agents', 536.9576671123505), ('env.step', 112.01809453964233), ('update goal', 5.838938474655151), ('store to buffer', 5.51747727394104), ('init episode', 0.8121154308319092), ('train coach', 0.5636041164398193), ('write log', 0.020383596420288086), ('save model', 0.0001723766326904297)]
============epi=75,step=21366,avg_reward=-0.350105757582301,goal_score=0==============
[('train agents', 547.3796639442444), ('env.step', 113.71207070350647), ('update goal', 5.931689262390137), ('store to buffer', 5.606595039367676), ('init episode', 0.8244428634643555), ('train coach', 0.5732631683349609), ('write log', 0.02062845230102539), ('save model', 0.00017571449279785156)]
============epi=76,step=21666,avg_reward=-0.5464316284621316,goal_score=0==============
[('train agents', 556.203882932663), ('env.step', 115.32102656364441), ('update goal', 6.01676607131958), ('store to buffer', 5.6983020305633545), ('init episode', 0.8369593620300293), ('train coach', 0.5808947086334229), ('write log', 0.02092576026916504), ('save model', 0.00017833709716796875)]
============epi=77,step=21966,avg_reward=-1.1849913608630198,goal_score=0==============
[('train agents', 564.1063666343689), ('env.step', 116.88265109062195), ('update goal', 6.099685192108154), ('store to buffer', 5.787165641784668), ('init episode', 0.8471760749816895), ('train coach', 0.5883269309997559), ('write log', 0.021205663681030273), ('save model', 0.0001804828643798828)]
============epi=78,step=22266,avg_reward=-0.5138793159093257,goal_score=0==============
[('train agents', 571.6427779197693), ('env.step', 118.49706768989563), ('update goal', 6.1766626834869385), ('store to buffer', 5.862504005432129), ('init episode', 0.8573586940765381), ('train coach', 0.5958552360534668), ('write log', 0.021508216857910156), ('save model', 0.00018286705017089844)]
============epi=79,step=22566,avg_reward=-0.45859132646027395,goal_score=0==============
[('train agents', 578.9174468517303), ('env.step', 120.103520154953), ('update goal', 6.250976324081421), ('store to buffer', 5.939072608947754), ('init episode', 0.8676316738128662), ('train coach', 0.6032669544219971), ('write log', 0.021741867065429688), ('save model', 0.0001850128173828125)]
============epi=80,step=22866,avg_reward=-0.751829171217926,goal_score=0==============
[('train agents', 586.2401874065399), ('env.step', 121.48798155784607), ('update goal', 6.327972173690796), ('store to buffer', 6.021795272827148), ('init episode', 0.8773846626281738), ('train coach', 0.6106503009796143), ('write log', 0.022069215774536133), ('save model', 0.0001876354217529297)]
============epi=81,step=23166,avg_reward=-0.7290911236390717,goal_score=0==============
[('train agents', 593.5092177391052), ('env.step', 122.85474681854248), ('update goal', 6.402510643005371), ('store to buffer', 6.091740608215332), ('init episode', 0.8875291347503662), ('train coach', 0.6182155609130859), ('write log', 0.022313356399536133), ('save model', 0.0001900196075439453)]
============epi=82,step=23466,avg_reward=-0.7952425843485642,goal_score=0==============
[('train agents', 600.802152633667), ('env.step', 124.30727863311768), ('update goal', 6.477001428604126), ('store to buffer', 6.159091472625732), ('init episode', 0.8976435661315918), ('train coach', 0.6257338523864746), ('write log', 0.022610902786254883), ('save model', 0.0001926422119140625)]
============epi=83,step=23766,avg_reward=-0.4406384512613562,goal_score=0==============
[('train agents', 608.1262848377228), ('env.step', 125.72752165794373), ('update goal', 6.554005861282349), ('store to buffer', 6.230374336242676), ('init episode', 0.9077646732330322), ('train coach', 0.6330106258392334), ('write log', 0.022879362106323242), ('save model', 0.000194549560546875)]
============epi=84,step=24066,avg_reward=-0.36346642970007487,goal_score=0==============
[('train agents', 615.4055862426758), ('env.step', 127.30184149742126), ('update goal', 6.629105091094971), ('store to buffer', 6.3086512088775635), ('init episode', 0.9175715446472168), ('train coach', 0.6405148506164551), ('write log', 0.0231320858001709), ('save model', 0.00019669532775878906)]
============epi=85,step=24366,avg_reward=-0.22151430129807811,goal_score=0==============
[('train agents', 622.6862325668335), ('env.step', 128.7012233734131), ('update goal', 6.70536732673645), ('store to buffer', 6.385328531265259), ('init episode', 0.9276478290557861), ('train coach', 0.6481301784515381), ('write log', 0.023425579071044922), ('save model', 0.00019931793212890625)]
============epi=86,step=24666,avg_reward=-0.7814193130932551,goal_score=0==============
[('train agents', 629.986034154892), ('env.step', 130.11459279060364), ('update goal', 6.781084299087524), ('store to buffer', 6.453755855560303), ('init episode', 0.9378337860107422), ('train coach', 0.655444860458374), ('write log', 0.023708105087280273), ('save model', 0.00020170211791992188)]
============epi=87,step=24966,avg_reward=-0.14167654429554574,goal_score=0==============
[('train agents', 637.2241644859314), ('env.step', 131.61419439315796), ('update goal', 6.854785203933716), ('store to buffer', 6.527477502822876), ('init episode', 0.9478826522827148), ('train coach', 0.6629700660705566), ('write log', 0.02399897575378418), ('save model', 0.00020384788513183594)]
============epi=88,step=25266,avg_reward=-0.23905474788836648,goal_score=0==============
[('train agents', 644.544951915741), ('env.step', 133.00898337364197), ('update goal', 6.929440021514893), ('store to buffer', 6.606673240661621), ('init episode', 0.9580967426300049), ('train coach', 0.6704046726226807), ('write log', 0.02427530288696289), ('save model', 0.00020623207092285156)]
============epi=89,step=25566,avg_reward=0.01709404998422635,goal_score=0==============
[('train agents', 651.7879121303558), ('env.step', 134.3630313873291), ('update goal', 7.003602981567383), ('store to buffer', 6.682892799377441), ('init episode', 0.968275785446167), ('train coach', 0.6779460906982422), ('write log', 0.024517297744750977), ('save model', 0.00020813941955566406)]
============epi=90,step=25866,avg_reward=-0.30414950731043394,goal_score=0==============
[('train agents', 659.1370565891266), ('env.step', 135.75312614440918), ('update goal', 7.079636335372925), ('store to buffer', 6.758374929428101), ('init episode', 0.9784774780273438), ('train coach', 0.685518741607666), ('write log', 0.02481245994567871), ('save model', 0.00021004676818847656)]
============epi=91,step=26166,avg_reward=-1.0311265498389108,goal_score=0==============
[('train agents', 666.8427445888519), ('env.step', 137.29971742630005), ('update goal', 7.157801866531372), ('store to buffer', 6.831547021865845), ('init episode', 0.9887354373931885), ('train coach', 0.6930835247039795), ('write log', 0.0251004695892334), ('save model', 0.00021266937255859375)]
============epi=92,step=26466,avg_reward=-0.11587325550588069,goal_score=0==============
[('train agents', 674.5844061374664), ('env.step', 138.7638816833496), ('update goal', 7.236985683441162), ('store to buffer', 6.912042617797852), ('init episode', 1.0002951622009277), ('train coach', 0.7016885280609131), ('write log', 0.025400876998901367), ('save model', 0.00021529197692871094)]
============epi=93,step=26766,avg_reward=-0.4942954410294407,goal_score=0==============
[('train agents', 682.2932140827179), ('env.step', 140.2547492980957), ('update goal', 7.314608335494995), ('store to buffer', 6.989592552185059), ('init episode', 1.011176347732544), ('train coach', 0.7090363502502441), ('write log', 0.02568364143371582), ('save model', 0.00021767616271972656)]
============epi=94,step=27066,avg_reward=-0.16454205521336307,goal_score=0==============
[('train agents', 690.4003422260284), ('env.step', 141.76086282730103), ('update goal', 7.397743463516235), ('store to buffer', 7.06576681137085), ('init episode', 1.0214667320251465), ('train coach', 0.7183051109313965), ('write log', 0.025920867919921875), ('save model', 0.0002200603485107422)]
============epi=95,step=27366,avg_reward=-0.5205093367325418,goal_score=0==============
[('train agents', 699.6949889659882), ('env.step', 143.46917128562927), ('update goal', 7.489095687866211), ('store to buffer', 7.154102563858032), ('init episode', 1.0341849327087402), ('train coach', 0.7264430522918701), ('write log', 0.02622818946838379), ('save model', 0.0002224445343017578)]
============epi=96,step=27666,avg_reward=-0.7875546937447557,goal_score=0==============
[('train agents', 708.1726114749908), ('env.step', 145.1193881034851), ('update goal', 7.573460578918457), ('store to buffer', 7.2345871925354), ('init episode', 1.046062707901001), ('train coach', 0.7361483573913574), ('write log', 0.026510953903198242), ('save model', 0.00022530555725097656)]
============epi=97,step=27966,avg_reward=-0.5776666689400022,goal_score=0==============
[('train agents', 716.6403319835663), ('env.step', 146.6273341178894), ('update goal', 7.660877466201782), ('store to buffer', 7.314127445220947), ('init episode', 1.059232473373413), ('train coach', 0.7453417778015137), ('write log', 0.02676105499267578), ('save model', 0.00022745132446289062)]
============epi=98,step=28266,avg_reward=-0.16299165625710765,goal_score=0==============
[('train agents', 725.4300725460052), ('env.step', 148.42148566246033), ('update goal', 7.746103763580322), ('store to buffer', 7.400619268417358), ('init episode', 1.071211338043213), ('train coach', 0.7529385089874268), ('write log', 0.027050495147705078), ('save model', 0.00022983551025390625)]
============epi=99,step=28566,avg_reward=-0.45700684661452434,goal_score=0==============
[('train agents', 734.1787078380585), ('env.step', 149.95643782615662), ('update goal', 7.835851430892944), ('store to buffer', 7.4828269481658936), ('init episode', 1.0812807083129883), ('train coach', 0.7607052326202393), ('write log', 0.02717757225036621), ('save model', 0.00023221969604492188)]
============epi=100,step=28866,avg_reward=-0.07633896761377984,goal_score=0==============
[('train agents', 743.387246131897), ('env.step', 151.56903100013733), ('update goal', 7.921590566635132), ('store to buffer', 7.567132234573364), ('init episode', 1.0919201374053955), ('train coach', 0.7682268619537354), ('write log', 0.02748250961303711), ('save model', 0.00023436546325683594)]
============epi=101,step=29164,avg_reward=-1.7169734522543432,goal_score=-1==============
[('train agents', 751.1334645748138), ('env.step', 153.01200318336487), ('update goal', 8.001579999923706), ('store to buffer', 7.643481492996216), ('init episode', 1.1022729873657227), ('train coach', 0.7761914730072021), ('write log', 0.027624130249023438), ('save model', 0.00023794174194335938)]
============epi=102,step=29464,avg_reward=-0.34677768289052724,goal_score=0==============
[('train agents', 759.1775867938995), ('env.step', 154.57448840141296), ('update goal', 8.080833911895752), ('store to buffer', 7.718963384628296), ('init episode', 1.112621545791626), ('train coach', 0.784010648727417), ('write log', 0.02787327766418457), ('save model', 0.000240325927734375)]
============epi=103,step=29764,avg_reward=-0.33644437735998795,goal_score=0==============
[('train agents', 768.6515600681305), ('env.step', 156.26457452774048), ('update goal', 8.172506332397461), ('store to buffer', 7.806653261184692), ('init episode', 1.1231911182403564), ('train coach', 0.7923734188079834), ('write log', 0.02820301055908203), ('save model', 0.00024271011352539062)]
============epi=104,step=30064,avg_reward=-0.48252900707697566,goal_score=0==============
[('train agents', 776.4669916629791), ('env.step', 157.74541187286377), ('update goal', 8.251803636550903), ('store to buffer', 7.889883756637573), ('init episode', 1.1352169513702393), ('train coach', 0.7999632358551025), ('write log', 0.028524398803710938), ('save model', 0.00024509429931640625)]
============epi=105,step=30364,avg_reward=-0.32020669801154117,goal_score=0==============
[('train agents', 784.4694867134094), ('env.step', 159.3892638683319), ('update goal', 8.33417558670044), ('store to buffer', 7.97598123550415), ('init episode', 1.1457765102386475), ('train coach', 0.8110191822052002), ('write log', 0.028841733932495117), ('save model', 0.0002486705780029297)]
============epi=106,step=30664,avg_reward=-0.3153363365271343,goal_score=0==============
[('train agents', 794.6910490989685), ('env.step', 161.2478392124176), ('update goal', 8.435753345489502), ('store to buffer', 8.069083213806152), ('init episode', 1.1612839698791504), ('train coach', 0.8228907585144043), ('write log', 0.029171228408813477), ('save model', 0.0002512931823730469)]
============epi=107,step=30844,avg_reward=-2.4937673148100523,goal_score=-1==============
[('train agents', 800.9222660064697), ('env.step', 162.32189011573792), ('update goal', 8.49683403968811), ('store to buffer', 8.12708330154419), ('init episode', 1.1774592399597168), ('train coach', 0.834606409072876), ('write log', 0.02949666976928711), ('save model', 0.0002543926239013672)]
============epi=108,step=31144,avg_reward=-0.4278222962579846,goal_score=0==============
[('train agents', 809.3520724773407), ('env.step', 163.82296180725098), ('update goal', 8.57999324798584), ('store to buffer', 8.21882152557373), ('init episode', 1.1924898624420166), ('train coach', 0.8421220779418945), ('write log', 0.029784679412841797), ('save model', 0.00025653839111328125)]
============epi=109,step=31444,avg_reward=-0.5128648854768463,goal_score=0==============
[('train agents', 818.0274469852448), ('env.step', 165.40087914466858), ('update goal', 8.667912006378174), ('store to buffer', 8.30700945854187), ('init episode', 1.2030389308929443), ('train coach', 0.8524038791656494), ('write log', 0.0299375057220459), ('save model', 0.00025916099548339844)]
============epi=110,step=31744,avg_reward=-0.2033657338930281,goal_score=0==============
[('train agents', 827.6405293941498), ('env.step', 167.05275535583496), ('update goal', 8.759171485900879), ('store to buffer', 8.399085998535156), ('init episode', 1.21690034866333), ('train coach', 0.8621840476989746), ('write log', 0.030203819274902344), ('save model', 0.0002617835998535156)]
============epi=111,step=32008,avg_reward=1.7426236250755576,goal_score=1==============
[('train agents', 836.1406891345978), ('env.step', 168.52579140663147), ('update goal', 8.838223218917847), ('store to buffer', 8.472527027130127), ('init episode', 1.2300822734832764), ('train coach', 0.8724439144134521), ('write log', 0.030488252639770508), ('save model', 0.0002639293670654297)]
============epi=112,step=32308,avg_reward=-0.40295260270585975,goal_score=0==============
[('train agents', 844.3041322231293), ('env.step', 170.064435005188), ('update goal', 8.92039942741394), ('store to buffer', 8.561529636383057), ('init episode', 1.24418044090271), ('train coach', 0.8812234401702881), ('write log', 0.030811309814453125), ('save model', 0.00026726722717285156)]
============epi=113,step=32608,avg_reward=-0.1851946293312545,goal_score=0==============
[('train agents', 852.4943552017212), ('env.step', 171.57220649719238), ('update goal', 9.00161361694336), ('store to buffer', 8.644434928894043), ('init episode', 1.2555186748504639), ('train coach', 0.8885037899017334), ('write log', 0.031140804290771484), ('save model', 0.0002696514129638672)]
============epi=114,step=32908,avg_reward=-0.30703343829549024,goal_score=0==============
[('train agents', 860.7575244903564), ('env.step', 173.2547664642334), ('update goal', 9.082685232162476), ('store to buffer', 8.732350826263428), ('init episode', 1.2678775787353516), ('train coach', 0.8961319923400879), ('write log', 0.031465768814086914), ('save model', 0.0002722740173339844)]
============epi=115,step=33208,avg_reward=-0.13674945495111446,goal_score=0==============
[('train agents', 870.1769607067108), ('env.step', 174.95131134986877), ('update goal', 9.180017709732056), ('store to buffer', 8.821022272109985), ('init episode', 1.279447317123413), ('train coach', 0.9058218002319336), ('write log', 0.03172183036804199), ('save model', 0.00027489662170410156)]
============epi=116,step=33508,avg_reward=0.23271359881361459,goal_score=0==============
[('train agents', 878.9518036842346), ('env.step', 176.61185312271118), ('update goal', 9.273651123046875), ('store to buffer', 8.904213905334473), ('init episode', 1.2923922538757324), ('train coach', 0.9140896797180176), ('write log', 0.03200984001159668), ('save model', 0.00027680397033691406)]
============epi=117,step=33684,avg_reward=1.5637712292067893,goal_score=1==============
[('train agents', 883.5269811153412), ('env.step', 177.46923637390137), ('update goal', 9.318475723266602), ('store to buffer', 8.953320264816284), ('init episode', 1.3025989532470703), ('train coach', 0.9214828014373779), ('write log', 0.03228402137756348), ('save model', 0.0002789497375488281)]
============epi=118,step=33984,avg_reward=-0.2850642062584615,goal_score=0==============
[('train agents', 892.8674294948578), ('env.step', 179.05563759803772), ('update goal', 9.469929933547974), ('store to buffer', 9.065393209457397), ('init episode', 1.3127896785736084), ('train coach', 0.9290013313293457), ('write log', 0.0325775146484375), ('save model', 0.0002808570861816406)]
============epi=119,step=34284,avg_reward=-0.3094105342783673,goal_score=0==============
[('train agents', 901.2968230247498), ('env.step', 180.58981227874756), ('update goal', 9.552458047866821), ('store to buffer', 9.14560866355896), ('init episode', 1.3229055404663086), ('train coach', 0.9382884502410889), ('write log', 0.032712459564208984), ('save model', 0.00028324127197265625)]
============epi=120,step=34584,avg_reward=-0.2165044318653397,goal_score=0==============
[('train agents', 911.7443051338196), ('env.step', 182.6034963130951), ('update goal', 9.650662422180176), ('store to buffer', 9.23722243309021), ('init episode', 1.3363587856292725), ('train coach', 0.9485890865325928), ('write log', 0.033066749572753906), ('save model', 0.0002856254577636719)]
============epi=121,step=34884,avg_reward=-0.19374772334308663,goal_score=0==============
[('train agents', 921.1076934337616), ('env.step', 184.2679204940796), ('update goal', 9.742467880249023), ('store to buffer', 9.324566125869751), ('init episode', 1.35127854347229), ('train coach', 0.9565451145172119), ('write log', 0.03337740898132324), ('save model', 0.0002875328063964844)]
============epi=122,step=35184,avg_reward=-0.45953078168697514,goal_score=0==============
[('train agents', 930.0412950515747), ('env.step', 185.87899160385132), ('update goal', 9.833123922348022), ('store to buffer', 9.415766716003418), ('init episode', 1.3617730140686035), ('train coach', 0.9640669822692871), ('write log', 0.03368782997131348), ('save model', 0.00029087066650390625)]
============epi=123,step=35391,avg_reward=-1.8844673144148731,goal_score=-1==============
[('train agents', 936.3044354915619), ('env.step', 186.97981786727905), ('update goal', 9.893804550170898), ('store to buffer', 9.478676080703735), ('init episode', 1.3732869625091553), ('train coach', 0.9713325500488281), ('write log', 0.033972740173339844), ('save model', 0.0002932548522949219)]
============epi=124,step=35477,avg_reward=3.9282758929090305,goal_score=1==============
[('train agents', 938.7537398338318), ('env.step', 187.43621516227722), ('update goal', 9.915730476379395), ('store to buffer', 9.50271725654602), ('init episode', 1.3832051753997803), ('train coach', 0.9812943935394287), ('write log', 0.0341639518737793), ('save model', 0.00029659271240234375)]
============epi=125,step=35777,avg_reward=-0.2396783746541037,goal_score=0==============
[('train agents', 947.3977980613708), ('env.step', 188.9473521709442), ('update goal', 10.000561952590942), ('store to buffer', 9.581902265548706), ('init episode', 1.3941686153411865), ('train coach', 0.9900870323181152), ('write log', 0.034287452697753906), ('save model', 0.00029850006103515625)]
============epi=126,step=36077,avg_reward=-0.3826012416996419,goal_score=0==============
[('train agents', 956.4373958110809), ('env.step', 190.57422137260437), ('update goal', 10.087287425994873), ('store to buffer', 9.663897037506104), ('init episode', 1.404754400253296), ('train coach', 0.9975447654724121), ('write log', 0.03456902503967285), ('save model', 0.0003008842468261719)]
============epi=127,step=36377,avg_reward=-0.9480287106484769,goal_score=0==============
[('train agents', 965.651166677475), ('env.step', 192.2149739265442), ('update goal', 10.177494525909424), ('store to buffer', 9.746994256973267), ('init episode', 1.4159586429595947), ('train coach', 1.004960060119629), ('write log', 0.0348663330078125), ('save model', 0.00030350685119628906)]
============epi=128,step=36677,avg_reward=-0.09584993144529078,goal_score=0==============
[('train agents', 974.6032056808472), ('env.step', 193.81865072250366), ('update goal', 10.261008501052856), ('store to buffer', 9.82596492767334), ('init episode', 1.426133394241333), ('train coach', 1.0162279605865479), ('write log', 0.03519153594970703), ('save model', 0.0003070831298828125)]
============epi=129,step=36977,avg_reward=-0.4589224181925568,goal_score=0==============
[('train agents', 983.7744302749634), ('env.step', 195.5020031929016), ('update goal', 10.353698015213013), ('store to buffer', 9.907458543777466), ('init episode', 1.44020676612854), ('train coach', 1.02476167678833), ('write log', 0.03548169136047363), ('save model', 0.0003094673156738281)]
============epi=130,step=37277,avg_reward=-0.4432435261441469,goal_score=0==============
[('train agents', 992.1839644908905), ('env.step', 197.37221765518188), ('update goal', 10.438848495483398), ('store to buffer', 9.990634679794312), ('init episode', 1.4504306316375732), ('train coach', 1.0321063995361328), ('write log', 0.035720109939575195), ('save model', 0.0003116130828857422)]
============epi=131,step=37577,avg_reward=-0.17393107633806995,goal_score=0==============
[('train agents', 999.7598524093628), ('env.step', 198.7345871925354), ('update goal', 10.514366865158081), ('store to buffer', 10.06717586517334), ('init episode', 1.4606730937957764), ('train coach', 1.03932785987854), ('write log', 0.03600287437438965), ('save model', 0.0003139972686767578)]
============epi=132,step=37877,avg_reward=-0.16480551920330477,goal_score=0==============
[('train agents', 1007.2209753990173), ('env.step', 200.10439038276672), ('update goal', 10.587678909301758), ('store to buffer', 10.14564323425293), ('init episode', 1.4703693389892578), ('train coach', 1.0469095706939697), ('write log', 0.03629016876220703), ('save model', 0.00031638145446777344)]
============epi=133,step=38015,avg_reward=2.8815070279105566,goal_score=1==============
[('train agents', 1010.8156807422638), ('env.step', 200.75535655021667), ('update goal', 10.621572256088257), ('store to buffer', 10.178252220153809), ('init episode', 1.48036789894104), ('train coach', 1.0543146133422852), ('write log', 0.03657341003417969), ('save model', 0.00031876564025878906)]
============epi=134,step=38315,avg_reward=-0.27345435069603974,goal_score=0==============
[('train agents', 1020.3118326663971), ('env.step', 202.33604764938354), ('update goal', 10.704257726669312), ('store to buffer', 10.261366844177246), ('init episode', 1.4908838272094727), ('train coach', 1.0653843879699707), ('write log', 0.036867380142211914), ('save model', 0.0003218650817871094)]
============epi=135,step=38572,avg_reward=1.0762846791914742,goal_score=1==============
[('train agents', 1029.4833798408508), ('env.step', 203.92372393608093), ('update goal', 10.795226097106934), ('store to buffer', 10.336824417114258), ('init episode', 1.5057263374328613), ('train coach', 1.0729682445526123), ('write log', 0.03717350959777832), ('save model', 0.0003237724304199219)]
============epi=136,step=38872,avg_reward=-0.4012485704079509,goal_score=0==============
[('train agents', 1038.1572182178497), ('env.step', 205.4378321170807), ('update goal', 10.882322311401367), ('store to buffer', 10.410772800445557), ('init episode', 1.5156846046447754), ('train coach', 1.0805189609527588), ('write log', 0.037474870681762695), ('save model', 0.00032639503479003906)]
============epi=137,step=39172,avg_reward=-0.5397378263895117,goal_score=0==============
[('train agents', 1046.743395805359), ('env.step', 206.98955607414246), ('update goal', 10.965982675552368), ('store to buffer', 10.48870301246643), ('init episode', 1.5257554054260254), ('train coach', 1.088249921798706), ('write log', 0.03777170181274414), ('save model', 0.0003287792205810547)]
============epi=138,step=39472,avg_reward=-0.7076840069166931,goal_score=0==============
[('train agents', 1055.5574011802673), ('env.step', 208.49506950378418), ('update goal', 11.046040058135986), ('store to buffer', 10.573723077774048), ('init episode', 1.5358984470367432), ('train coach', 1.0955133438110352), ('write log', 0.038053274154663086), ('save model', 0.0003306865692138672)]
============epi=139,step=39772,avg_reward=-0.5157850436775241,goal_score=0==============
[('train agents', 1063.3266599178314), ('env.step', 210.21019315719604), ('update goal', 11.123029708862305), ('store to buffer', 10.649738788604736), ('init episode', 1.5455610752105713), ('train coach', 1.1030402183532715), ('write log', 0.03834843635559082), ('save model', 0.0003333091735839844)]
============epi=140,step=40072,avg_reward=-0.31178322923983104,goal_score=0==============
[('train agents', 1070.9125008583069), ('env.step', 211.76894354820251), ('update goal', 11.200471878051758), ('store to buffer', 10.728945016860962), ('init episode', 1.5556252002716064), ('train coach', 1.1105360984802246), ('write log', 0.03863978385925293), ('save model', 0.00033545494079589844)]
============epi=141,step=40219,avg_reward=-2.8071116551230286,goal_score=-1==============
[('train agents', 1074.5688276290894), ('env.step', 212.42684078216553), ('update goal', 11.236708641052246), ('store to buffer', 10.767011165618896), ('init episode', 1.5657777786254883), ('train coach', 1.117828607559204), ('write log', 0.03892374038696289), ('save model', 0.0003376007080078125)]
============epi=142,step=40519,avg_reward=-0.6911514137094792,goal_score=0==============
[('train agents', 1082.0451364517212), ('env.step', 213.9087677001953), ('update goal', 11.310340881347656), ('store to buffer', 10.84363079071045), ('init episode', 1.5756194591522217), ('train coach', 1.1252751350402832), ('write log', 0.039159297943115234), ('save model', 0.00033974647521972656)]
============epi=143,step=40552,avg_reward=-11.066029938921652,goal_score=-1==============
[('train agents', 1082.854638338089), ('env.step', 214.0548996925354), ('update goal', 11.318007707595825), ('store to buffer', 10.851215362548828), ('init episode', 1.585799217224121), ('train coach', 1.1327993869781494), ('write log', 0.0393986701965332), ('save model', 0.0003421306610107422)]
============epi=144,step=40852,avg_reward=-0.32085778653273694,goal_score=0==============
[('train agents', 1090.3705525398254), ('env.step', 215.47294855117798), ('update goal', 11.391174554824829), ('store to buffer', 10.931186199188232), ('init episode', 1.5958490371704102), ('train coach', 1.1420352458953857), ('write log', 0.03963637351989746), ('save model', 0.0003445148468017578)]
============epi=145,step=41152,avg_reward=-0.6065171794971671,goal_score=0==============
[('train agents', 1097.8468170166016), ('env.step', 217.10804533958435), ('update goal', 11.46541166305542), ('store to buffer', 11.007127046585083), ('init episode', 1.6075892448425293), ('train coach', 1.149660348892212), ('write log', 0.039876699447631836), ('save model', 0.00034689903259277344)]
============epi=146,step=41212,avg_reward=-5.931079093185175,goal_score=-1==============
[('train agents', 1099.345142364502), ('env.step', 217.38998794555664), ('update goal', 11.478902816772461), ('store to buffer', 11.022254943847656), ('init episode', 1.6176536083221436), ('train coach', 1.157027244567871), ('write log', 0.04016470909118652), ('save model', 0.00034928321838378906)]
============epi=147,step=41512,avg_reward=-0.12033201895347129,goal_score=0==============
[('train agents', 1106.7716307640076), ('env.step', 218.7609157562256), ('update goal', 11.552392482757568), ('store to buffer', 11.09628415107727), ('init episode', 1.6276938915252686), ('train coach', 1.164536952972412), ('write log', 0.04040408134460449), ('save model', 0.0003521442413330078)]
============epi=148,step=41647,avg_reward=-3.309263069598776,goal_score=-1==============
[('train agents', 1110.1671035289764), ('env.step', 219.36889505386353), ('update goal', 11.585064172744751), ('store to buffer', 11.128949880599976), ('init episode', 1.637730598449707), ('train coach', 1.171513319015503), ('write log', 0.040679931640625), ('save model', 0.0003540515899658203)]
============epi=149,step=41947,avg_reward=-0.09622846984510257,goal_score=0==============
[('train agents', 1117.6378173828125), ('env.step', 220.77471327781677), ('update goal', 11.658745050430298), ('store to buffer', 11.20427131652832), ('init episode', 1.647282600402832), ('train coach', 1.1790027618408203), ('write log', 0.04092121124267578), ('save model', 0.0003561973571777344)]
============epi=150,step=42247,avg_reward=-0.6987578107547937,goal_score=0==============
[('train agents', 1125.0940759181976), ('env.step', 222.14340472221375), ('update goal', 11.732328414916992), ('store to buffer', 11.28326416015625), ('init episode', 1.6574480533599854), ('train coach', 1.1862366199493408), ('write log', 0.041150569915771484), ('save model', 0.00035858154296875)]
============epi=151,step=42547,avg_reward=-0.5495749658776383,goal_score=0==============
[('train agents', 1132.58469581604), ('env.step', 223.52005863189697), ('update goal', 11.80555772781372), ('store to buffer', 11.358522653579712), ('init episode', 1.6672155857086182), ('train coach', 1.193742036819458), ('write log', 0.041440725326538086), ('save model', 0.0003609657287597656)]
============epi=152,step=42847,avg_reward=-0.19891655878607054,goal_score=0==============
[('train agents', 1140.128363609314), ('env.step', 224.8997504711151), ('update goal', 11.878695011138916), ('store to buffer', 11.434221267700195), ('init episode', 1.6772451400756836), ('train coach', 1.20121169090271), ('write log', 0.04175519943237305), ('save model', 0.00036334991455078125)]
============epi=153,step=42911,avg_reward=6.00202061735189,goal_score=1==============
[('train agents', 1141.7327585220337), ('env.step', 225.1831214427948), ('update goal', 11.89394760131836), ('store to buffer', 11.449198722839355), ('init episode', 1.6874394416809082), ('train coach', 1.2084698677062988), ('write log', 0.042043209075927734), ('save model', 0.0003657341003417969)]
============epi=154,step=43211,avg_reward=-0.253369923727739,goal_score=0==============
[('train agents', 1149.2630388736725), ('env.step', 226.52652835845947), ('update goal', 11.966633796691895), ('store to buffer', 11.524011373519897), ('init episode', 1.6977076530456543), ('train coach', 1.2159628868103027), ('write log', 0.04233717918395996), ('save model', 0.0003681182861328125)]
============epi=155,step=43511,avg_reward=-0.36586612644476946,goal_score=0==============
[('train agents', 1156.7092661857605), ('env.step', 227.8958225250244), ('update goal', 12.039344549179077), ('store to buffer', 11.598323106765747), ('init episode', 1.70794677734375), ('train coach', 1.2235474586486816), ('write log', 0.042649030685424805), ('save model', 0.0003707408905029297)]
============epi=156,step=43638,avg_reward=-3.372866738951591,goal_score=-1==============
[('train agents', 1159.8831567764282), ('env.step', 228.47215819358826), ('update goal', 12.06942343711853), ('store to buffer', 11.631590366363525), ('init episode', 1.7182788848876953), ('train coach', 1.230886459350586), ('write log', 0.04293203353881836), ('save model', 0.0003733634948730469)]
============epi=157,step=43938,avg_reward=-0.4455569308949391,goal_score=0==============
[('train agents', 1167.3463101387024), ('env.step', 229.8544797897339), ('update goal', 12.14391827583313), ('store to buffer', 11.70596170425415), ('init episode', 1.7284016609191895), ('train coach', 1.2381823062896729), ('write log', 0.04321479797363281), ('save model', 0.0003757476806640625)]
============epi=158,step=44238,avg_reward=-0.0776053769123788,goal_score=0==============
[('train agents', 1174.8762674331665), ('env.step', 231.22665452957153), ('update goal', 12.21754789352417), ('store to buffer', 11.782368421554565), ('init episode', 1.7381582260131836), ('train coach', 1.245530366897583), ('write log', 0.04349064826965332), ('save model', 0.0003781318664550781)]
============epi=159,step=44538,avg_reward=-0.23760050113450945,goal_score=0==============
[('train agents', 1182.3202579021454), ('env.step', 232.6500804424286), ('update goal', 12.291940450668335), ('store to buffer', 11.86858582496643), ('init episode', 1.747953176498413), ('train coach', 1.2526240348815918), ('write log', 0.043753623962402344), ('save model', 0.0003802776336669922)]
============epi=160,step=44838,avg_reward=-0.3057473100963064,goal_score=0==============
[('train agents', 1189.7928504943848), ('env.step', 234.0642054080963), ('update goal', 12.366902351379395), ('store to buffer', 11.938121318817139), ('init episode', 1.7577688694000244), ('train coach', 1.2601139545440674), ('write log', 0.04401731491088867), ('save model', 0.0003826618194580078)]
============epi=161,step=44961,avg_reward=-2.7498529912656156,goal_score=-1==============
[('train agents', 1192.8377735614777), ('env.step', 234.67112469673157), ('update goal', 12.397377967834473), ('store to buffer', 11.97178339958191), ('init episode', 1.7679369449615479), ('train coach', 1.2677028179168701), ('write log', 0.04430341720581055), ('save model', 0.0003848075866699219)]
============epi=162,step=45261,avg_reward=-0.18919035567896214,goal_score=0==============
[('train agents', 1200.369570016861), ('env.step', 236.01316833496094), ('update goal', 12.471786975860596), ('store to buffer', 12.040887832641602), ('init episode', 1.7778918743133545), ('train coach', 1.274972915649414), ('write log', 0.04458260536193848), ('save model', 0.0003867149353027344)]
============epi=163,step=45561,avg_reward=-0.10398375744777077,goal_score=0==============
[('train agents', 1207.8985476493835), ('env.step', 237.35622239112854), ('update goal', 12.544694662094116), ('store to buffer', 12.115368843078613), ('init episode', 1.7877070903778076), ('train coach', 1.2822966575622559), ('write log', 0.04483938217163086), ('save model', 0.00038886070251464844)]
============epi=164,step=45729,avg_reward=-2.568815921207568,goal_score=-1==============
[('train agents', 1212.1517226696014), ('env.step', 238.10410380363464), ('update goal', 12.585369348526001), ('store to buffer', 12.157655239105225), ('init episode', 1.797530174255371), ('train coach', 1.289520502090454), ('write log', 0.045098066329956055), ('save model', 0.00039076805114746094)]
============epi=165,step=46029,avg_reward=-0.4793761856399465,goal_score=0==============
[('train agents', 1219.7121758460999), ('env.step', 239.5337233543396), ('update goal', 12.659172534942627), ('store to buffer', 12.234238624572754), ('init episode', 1.8072993755340576), ('train coach', 1.2970705032348633), ('write log', 0.04538846015930176), ('save model', 0.00039315223693847656)]
============epi=166,step=46329,avg_reward=-0.1922293176411024,goal_score=0==============
[('train agents', 1227.267494916916), ('env.step', 240.88121962547302), ('update goal', 12.731943368911743), ('store to buffer', 12.308204650878906), ('init episode', 1.8175230026245117), ('train coach', 1.304391622543335), ('write log', 0.04568934440612793), ('save model', 0.0003960132598876953)]
============epi=167,step=46629,avg_reward=-0.28174724361073555,goal_score=0==============
[('train agents', 1234.7534449100494), ('env.step', 242.22819828987122), ('update goal', 12.804640293121338), ('store to buffer', 12.386918067932129), ('init episode', 1.8274204730987549), ('train coach', 1.311913251876831), ('write log', 0.04592776298522949), ('save model', 0.0003986358642578125)]
============epi=168,step=46929,avg_reward=-0.12067228970234434,goal_score=0==============
[('train agents', 1242.245839357376), ('env.step', 243.57571411132812), ('update goal', 12.87791395187378), ('store to buffer', 12.46318531036377), ('init episode', 1.8376846313476562), ('train coach', 1.3191008567810059), ('write log', 0.046211957931518555), ('save model', 0.00040078163146972656)]
============epi=169,step=47229,avg_reward=-0.35271462999746017,goal_score=0==============
[('train agents', 1249.799180984497), ('env.step', 244.99489188194275), ('update goal', 12.950472116470337), ('store to buffer', 12.537711143493652), ('init episode', 1.8475115299224854), ('train coach', 1.3263916969299316), ('write log', 0.046495676040649414), ('save model', 0.0004029273986816406)]
============epi=170,step=47445,avg_reward=1.1650561385074758,goal_score=1==============
[('train agents', 1255.1997346878052), ('env.step', 246.06496906280518), ('update goal', 13.003230094909668), ('store to buffer', 12.591262102127075), ('init episode', 1.857365608215332), ('train coach', 1.333648681640625), ('write log', 0.04672837257385254), ('save model', 0.0004057884216308594)]
============epi=171,step=47745,avg_reward=0.24192168128201605,goal_score=0==============
[('train agents', 1262.7889964580536), ('env.step', 247.53820395469666), ('update goal', 13.075689792633057), ('store to buffer', 12.666814088821411), ('init episode', 1.8672850131988525), ('train coach', 1.3413667678833008), ('write log', 0.046985626220703125), ('save model', 0.000408172607421875)]
============epi=172,step=48045,avg_reward=-0.34644802066674507,goal_score=0==============
[('train agents', 1270.34104347229), ('env.step', 248.92936873435974), ('update goal', 13.14862847328186), ('store to buffer', 12.741451501846313), ('init episode', 1.8775646686553955), ('train coach', 1.3489408493041992), ('write log', 0.047310829162597656), ('save model', 0.00041031837463378906)]
============epi=173,step=48218,avg_reward=-2.1973675394537713,goal_score=-1==============
[('train agents', 1274.7277071475983), ('env.step', 249.70898485183716), ('update goal', 13.191376686096191), ('store to buffer', 12.78303575515747), ('init episode', 1.8901598453521729), ('train coach', 1.3565242290496826), ('write log', 0.04759788513183594), ('save model', 0.0004127025604248047)]
============epi=174,step=48518,avg_reward=-0.35029304242440173,goal_score=0==============
[('train agents', 1282.6757922172546), ('env.step', 251.18626022338867), ('update goal', 13.269794225692749), ('store to buffer', 12.859705924987793), ('init episode', 1.9003291130065918), ('train coach', 1.364091396331787), ('write log', 0.04793071746826172), ('save model', 0.00041556358337402344)]
============epi=175,step=48818,avg_reward=-0.26575189402431987,goal_score=0==============
[('train agents', 1290.754724740982), ('env.step', 252.6131145954132), ('update goal', 13.347808837890625), ('store to buffer', 12.936805009841919), ('init episode', 1.9131197929382324), ('train coach', 1.371495008468628), ('write log', 0.04820752143859863), ('save model', 0.00041747093200683594)]
============epi=176,step=48910,avg_reward=4.378788683468741,goal_score=1==============
[('train agents', 1293.6185224056244), ('env.step', 253.1063358783722), ('update goal', 13.37819218635559), ('store to buffer', 12.96067190170288), ('init episode', 1.9232902526855469), ('train coach', 1.3819823265075684), ('write log', 0.048483848571777344), ('save model', 0.0004208087921142578)]
============epi=177,step=49210,avg_reward=-0.34960081361900347,goal_score=0==============
[('train agents', 1301.905683517456), ('env.step', 254.77903580665588), ('update goal', 13.454909324645996), ('store to buffer', 13.032827138900757), ('init episode', 1.9363863468170166), ('train coach', 1.389221429824829), ('write log', 0.04874396324157715), ('save model', 0.00042319297790527344)]
============epi=178,step=49510,avg_reward=-0.14541682577455023,goal_score=0==============
[('train agents', 1310.7219665050507), ('env.step', 256.2812089920044), ('update goal', 13.537572145462036), ('store to buffer', 13.11455750465393), ('init episode', 1.946302890777588), ('train coach', 1.4013702869415283), ('write log', 0.04890179634094238), ('save model', 0.0004260540008544922)]
============epi=179,step=49589,avg_reward=4.148038532458501,goal_score=1==============
[('train agents', 1314.0754225254059), ('env.step', 256.79947996139526), ('update goal', 13.5682852268219), ('store to buffer', 13.141270160675049), ('init episode', 1.9622204303741455), ('train coach', 1.4089789390563965), ('write log', 0.04903817176818848), ('save model', 0.00042819976806640625)]
============epi=180,step=49793,avg_reward=1.8611821361168066,goal_score=1==============
[('train agents', 1319.6141681671143), ('env.step', 257.78586602211), ('update goal', 13.622771501541138), ('store to buffer', 13.197771787643433), ('init episode', 1.9723567962646484), ('train coach', 1.4188671112060547), ('write log', 0.049289703369140625), ('save model', 0.0004303455352783203)]
============epi=181,step=50093,avg_reward=0.2883010875218956,goal_score=0==============
[('train agents', 1328.5778856277466), ('env.step', 259.36911273002625), ('update goal', 13.708099126815796), ('store to buffer', 13.278284072875977), ('init episode', 1.9862010478973389), ('train coach', 1.426377534866333), ('write log', 0.049578189849853516), ('save model', 0.0004322528839111328)]
============epi=182,step=50393,avg_reward=-0.1314597145233019,goal_score=0==============
[('train agents', 1337.1855580806732), ('env.step', 260.93887281417847), ('update goal', 13.78958535194397), ('store to buffer', 13.359591484069824), ('init episode', 1.996422290802002), ('train coach', 1.4351603984832764), ('write log', 0.049704790115356445), ('save model', 0.0004341602325439453)]

ODE Message 3: LCP internal error, s <= 0 (s=-0.0000e+00)
============epi=183,step=50693,avg_reward=-0.489662907769947,goal_score=0==============
[('train agents', 1347.7085132598877), ('env.step', 262.66465401649475), ('update goal', 13.888672590255737), ('store to buffer', 13.444794654846191), ('init episode', 2.0067203044891357), ('train coach', 1.443828821182251), ('write log', 0.050022125244140625), ('save model', 0.0004363059997558594)]
============epi=184,step=50993,avg_reward=-0.33323222141967906,goal_score=0==============
[('train agents', 1356.2323625087738), ('env.step', 264.159556388855), ('update goal', 13.96953296661377), ('store to buffer', 13.518247604370117), ('init episode', 2.0180776119232178), ('train coach', 1.4511961936950684), ('write log', 0.05031704902648926), ('save model', 0.0004379749298095703)]
============epi=185,step=51293,avg_reward=-0.2760927838260447,goal_score=0==============
[('train agents', 1364.6879930496216), ('env.step', 265.6311402320862), ('update goal', 14.050292491912842), ('store to buffer', 13.594386339187622), ('init episode', 2.0281717777252197), ('train coach', 1.460524320602417), ('write log', 0.05045270919799805), ('save model', 0.00044083595275878906)]
============epi=186,step=51593,avg_reward=-0.5546479494647436,goal_score=0==============
[('train agents', 1373.403032541275), ('env.step', 267.22473526000977), ('update goal', 14.133885383605957), ('store to buffer', 13.668432474136353), ('init episode', 2.0401065349578857), ('train coach', 1.4678075313568115), ('write log', 0.0507814884185791), ('save model', 0.00044417381286621094)]
============epi=187,step=51630,avg_reward=-10.841169833579649,goal_score=-1==============
[('train agents', 1374.4120919704437), ('env.step', 267.40342259407043), ('update goal', 14.141837120056152), ('store to buffer', 13.678977489471436), ('init episode', 2.0499167442321777), ('train coach', 1.4754910469055176), ('write log', 0.051108360290527344), ('save model', 0.0004470348358154297)]
============epi=188,step=51930,avg_reward=-0.5927098937968597,goal_score=0==============
[('train agents', 1384.3775136470795), ('env.step', 269.04076528549194), ('update goal', 14.22932243347168), ('store to buffer', 13.76177167892456), ('init episode', 2.0602755546569824), ('train coach', 1.4837191104888916), ('write log', 0.0514070987701416), ('save model', 0.0004496574401855469)]
============epi=189,step=52206,avg_reward=1.3194357304258106,goal_score=1==============
[('train agents', 1392.191987991333), ('env.step', 270.46758222579956), ('update goal', 14.306949853897095), ('store to buffer', 13.839702367782593), ('init episode', 2.0708703994750977), ('train coach', 1.492929458618164), ('write log', 0.051737308502197266), ('save model', 0.00045228004455566406)]
============epi=190,step=52506,avg_reward=-0.11923694826084902,goal_score=0==============
[('train agents', 1403.0576241016388), ('env.step', 272.1195294857025), ('update goal', 14.40934705734253), ('store to buffer', 13.931804895401001), ('init episode', 2.0825560092926025), ('train coach', 1.5033867359161377), ('write log', 0.05205249786376953), ('save model', 0.0004546642303466797)]
============epi=191,step=52770,avg_reward=1.3934692277320497,goal_score=1==============
[('train agents', 1410.7201261520386), ('env.step', 273.416588306427), ('update goal', 14.484747171401978), ('store to buffer', 14.006740093231201), ('init episode', 2.096452474594116), ('train coach', 1.5132911205291748), ('write log', 0.05236530303955078), ('save model', 0.00045680999755859375)]
============epi=192,step=53070,avg_reward=-0.7096655984493014,goal_score=0==============
[('train agents', 1419.6143233776093), ('env.step', 274.89041781425476), ('update goal', 14.567875862121582), ('store to buffer', 14.089288711547852), ('init episode', 2.1099369525909424), ('train coach', 1.5209412574768066), ('write log', 0.052655696868896484), ('save model', 0.0004589557647705078)]
============epi=193,step=53370,avg_reward=-0.5307538005514434,goal_score=0==============
[('train agents', 1429.3085911273956), ('env.step', 276.5903251171112), ('update goal', 14.655653715133667), ('store to buffer', 14.180508375167847), ('init episode', 2.120460271835327), ('train coach', 1.5314161777496338), ('write log', 0.05292057991027832), ('save model', 0.00046133995056152344)]
============epi=194,step=53670,avg_reward=-0.7822430869988559,goal_score=0==============
[('train agents', 1439.43892288208), ('env.step', 278.259530544281), ('update goal', 14.74714469909668), ('store to buffer', 14.269435167312622), ('init episode', 2.134251356124878), ('train coach', 1.539278507232666), ('write log', 0.05322742462158203), ('save model', 0.00046372413635253906)]
============epi=195,step=53734,avg_reward=-5.4369208721397415,goal_score=-1==============
[('train agents', 1441.5614686012268), ('env.step', 278.61020612716675), ('update goal', 14.765594720840454), ('store to buffer', 14.288438558578491), ('init episode', 2.144785165786743), ('train coach', 1.5475273132324219), ('write log', 0.05352520942687988), ('save model', 0.0004665851593017578)]
============epi=196,step=54034,avg_reward=-0.43585070587520686,goal_score=0==============
[('train agents', 1451.61674284935), ('env.step', 280.28271746635437), ('update goal', 14.859108448028564), ('store to buffer', 14.379740953445435), ('init episode', 2.156078815460205), ('train coach', 1.5574414730072021), ('write log', 0.05377054214477539), ('save model', 0.00046896934509277344)]
============epi=197,step=54187,avg_reward=2.5317781702823465,goal_score=1==============
[('train agents', 1456.0529026985168), ('env.step', 281.05849051475525), ('update goal', 14.901337146759033), ('store to buffer', 14.418498992919922), ('init episode', 2.167515754699707), ('train coach', 1.5674412250518799), ('write log', 0.05402946472167969), ('save model', 0.00047135353088378906)]
============epi=198,step=54240,avg_reward=-6.9735301536560135,goal_score=-1==============
[('train agents', 1457.6773025989532), ('env.step', 281.3238980770111), ('update goal', 14.916486978530884), ('store to buffer', 14.431916952133179), ('init episode', 2.1790060997009277), ('train coach', 1.5750987529754639), ('write log', 0.05431342124938965), ('save model', 0.00047326087951660156)]
============epi=199,step=54540,avg_reward=-0.3130981436248716,goal_score=0==============
[('train agents', 1466.799281835556), ('env.step', 283.451669216156), ('update goal', 14.999090671539307), ('store to buffer', 14.539191007614136), ('init episode', 2.189049243927002), ('train coach', 1.58543062210083), ('write log', 0.05460953712463379), ('save model', 0.0004756450653076172)]
Traceback (most recent call last):
  File "/home/user/football/HRL/MAIN.py", line 219, in <module>
  File "/home/user/football/HRL/MAIN.py", line 134, in run
    self.timer.start_timer("write log")
  File "/home/user/football/HRL/matd3.py", line 96, in save_model
    os.mkdir("./model/{}".format(env_name))
FileNotFoundError: [Errno 2] No such file or directory: './model/VSSMA-v0'

Process finished with exit code 1
